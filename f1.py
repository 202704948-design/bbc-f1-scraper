import requests  # å¯¼å…¥ç½‘ç»œè¯·æ±‚å·¥å…·ï¼Œç”¨äºè·å–ç½‘é¡µæºä»£ç 
from bs4 import BeautifulSoup  # å¯¼å…¥ç½‘é¡µè§£æå·¥å…·ï¼Œç”¨äºæå– HTML æ•°æ®
import csv  # å¯¼å…¥ CSV å·¥å…·ï¼Œç”¨äºè¯»å†™è¡¨æ ¼æ–‡ä»¶
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼å·¥å…·ï¼Œç”¨äºå¤„ç†æ–‡æœ¬åŒ¹é…
import os  # å¯¼å…¥ç³»ç»Ÿå·¥å…·ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
import smtplib  # å¯¼å…¥é‚®ä»¶åè®®å·¥å…·ï¼Œè´Ÿè´£å‘é€é‚®ä»¶
from email.mime.text import MIMEText  # å¯¼å…¥é‚®ä»¶æ ¼å¼å·¥å…·ï¼Œç”¨äºç¼–å†™é‚®ä»¶æ­£æ–‡

# ==========================================
# 1. åŸºç¡€é…ç½®ï¼ˆæ¯”èµ›å‘è½¦ä½ï¼‰
# ==========================================
url = "https://www.bbc.com/sport/formula1"  # ç›®æ ‡ç½‘å€ï¼šBBC F1 é¢‘é“
headers = {'User-Agent': 'Mozilla/5.0'}  # ä¼ªè£…æˆæµè§ˆå™¨ï¼Œé¿å…è¢«ç½‘ç«™æ‹¦æˆª
filename = 'f1_news_perfect.csv'  # å­˜å‚¨æ•°æ®çš„æ–‡ä»¶å

# å®šä¹‰æ—¶é—´å…³é”®è¯ï¼Œç”¨äºåœ¨å¤æ‚çš„ç½‘é¡µæ ‡ç­¾ä¸­ç²¾å‡†è¯†åˆ«å‡ºâ€œæ—¶é—´â€
time_keywords = [
    "posted", "ago", "hours", "mins", "days", "year", "available",
    "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec"
]

# ==========================================
# 2. å»ºç«‹è®°å¿†ï¼ˆæŸ¥çœ‹æ˜¯å¦æœ‰æ—§æ•°æ®ï¼‰
# ==========================================
old_titles = set()  # åˆ›å»ºä¸€ä¸ªé›†åˆæ¥å­˜æ”¾æ—§æ ‡é¢˜ï¼ˆé›†åˆæŸ¥æ‰¾é€Ÿåº¦æå¿«ä¸”è‡ªåŠ¨å»é‡ï¼‰
if os.path.exists(filename):  # å¦‚æœ CSV æ–‡ä»¶å·²ç»å­˜åœ¨
    with open(filename, 'r', encoding='utf-8-sig') as f:  # ä»¥è¯»å–æ¨¡å¼æ‰“å¼€æ–‡ä»¶
        reader = csv.reader(f)  # åˆ›å»ºé˜…è¯»å™¨
        try:
            next(reader)  # è·³è¿‡ç¬¬ä¸€è¡Œçš„è¡¨å¤´
            for row in reader:  # éå†æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œ
                if len(row) > 2:  # ç¡®ä¿è¿™è¡Œæ•°æ®å®Œæ•´ï¼ˆæ ‡é¢˜é€šå¸¸åœ¨ç¬¬ 3 åˆ—ï¼‰
                    old_titles.add(row[2])  # å°†æ—§æ ‡é¢˜å­˜å…¥é›†åˆï¼Œæ–¹ä¾¿åé¢å¯¹æ¯”
        except StopIteration:  # å¦‚æœæ–‡ä»¶æ˜¯ç©ºçš„ï¼Œç›´æ¥è·³è¿‡
            pass

# ==========================================
# 3. ç°åœºæŠ“å–ï¼ˆæ·±å…¥å›´åœºï¼‰
# ==========================================
response = requests.get(url, headers=headers)  # å‘ç½‘ç«™å‘é€è¯·æ±‚
if response.status_code == 200:  # å¦‚æœæœåŠ¡å™¨è¿”å› 200ï¼ˆä»£è¡¨æˆåŠŸæ‰“å¼€ç½‘é¡µï¼‰
    print("æˆåŠŸè¿æ¥åˆ° BBCï¼æ­£åœ¨è§£ææ•°æ®...")
    soup = BeautifulSoup(response.text, "html.parser")  # å°†ç½‘é¡µæºä»£ç äº¤ç»™ç¿»è¯‘å®˜è§£æ
else:
    print(f"é—¨æ²¡å¼€ï¼é”™è¯¯ä»£ç ï¼š{response.status_code}")
    exit()  # å¦‚æœè¿æ¥å¤±è´¥ï¼Œç›´æ¥ç»“æŸç¨‹åº

boxes = soup.find_all("div", attrs={"data-testid": "promo"})  # æ‰¾åˆ°ç½‘é¡µä¸­æ‰€æœ‰çš„æ–°é—»å°ç›’å­
current_scraped_data = []  # å‡†å¤‡ä¸€ä¸ªç¯®å­ï¼Œè£…ä»Šå¤©æŠ“åˆ°çš„æ‰€æœ‰æ–°é—»
new_stories_for_email = []  # å‡†å¤‡å¦ä¸€ä¸ªç¯®å­ï¼Œä¸“é—¨è£…æ—§è®°å½•é‡Œæ²¡æœ‰çš„â€œæ–°é²œäº‹â€

for box in boxes:  # å¼€å§‹é€ä¸ªè§£å‰–æ–°é—»ç›’å­
    # --- A. æå–æ ‡é¢˜ä¸é“¾æ¥ ---
    a_tag = box.find("a")  # åœ¨ç›’å­é‡Œæ‰¾é“¾æ¥æ ‡ç­¾ <a>
    title = a_tag.get_text(strip=True) if a_tag else "æ— æ ‡é¢˜"  # æå–æ–‡å­—ï¼Œé¡ºä¾¿å‰ªæ‰å¤šä½™ç©ºæ ¼
    raw_link = a_tag.get('href', '') if a_tag else ""  # æ‹¿åˆ°åŸå§‹é“¾æ¥
    link = f"https://www.bbc.com{raw_link}" if raw_link.startswith('/') else raw_link  # è¡¥å…¨ BBC åŸŸå
    
    # --- B. æå–ç®€è®¯ ---
    summary_tag = box.find("p", class_="ssrcss-1q0x1qg-Paragraph")  # å¯»æ‰¾ç‰¹å®šç±»åçš„æ®µè½
    summary = summary_tag.get_text(strip=True) if summary_tag else "ï¼ˆç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…ï¼‰"
    
    # --- C. å¤„ç†æ—¶é—´ä¸åˆ†ç±» ---
    metadata_spans = box.find_all("span", class_="ssrcss-61mhsj-MetadataText e4wm5bw1")
    post_time, other_info = "", []  # åˆå§‹åŒ–æ—¶é—´å’Œåˆ†ç±»å˜é‡
    
    for span in metadata_spans:  # éå†ç›’å­é‡Œçš„æ¯ä¸ªå°é›¶ä»¶
        full_text = span.get_text(strip=True)
        text_lower = full_text.lower()
        
        if "comment" in text_lower or "follow" in text_lower:
            continue  # å¿½ç•¥â€œè¯„è®ºâ€æˆ–â€œå…³æ³¨â€ä¹‹ç±»çš„å¹²æ‰°é¡¹
            
        is_time = any(word in text_lower for word in time_keywords)  # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´å…³é”®è¯
        if is_time:
            hidden_tag = span.find("span", class_="visually-hidden")  # ä¼˜å…ˆæ‰¾éšè—çš„å¹²å‡€æ—¥æœŸ
            post_time = hidden_tag.get_text(strip=True) if hidden_tag else full_text[:len(full_text)//2]
        else:
            if full_text and not full_text.isdigit():  # å¦‚æœä¸æ˜¯æ—¶é—´ä¹Ÿä¸æ˜¯çº¯æ•°å­—ï¼Œå°±æ˜¯åˆ†ç±»ä¿¡æ¯
                other_info.append(full_text)
    
    category = " | ".join(list(dict.fromkeys(other_info))) if other_info else "Formula 1"
    
    # --- D. å¯¹æ¯”ï¼šè¿™æ˜¯æ–°å‡ºçš„æ–°é—»å—ï¼Ÿ ---
    if title not in old_titles and title != "æ— æ ‡é¢˜":
        new_stories_for_email.append({"title": title, "link": link})  # æ”¾å…¥å¾…æé†’ç¯®å­
    
    current_scraped_data.append([post_time, category, title, summary])  # æ”¾å…¥å¾…ä¿å­˜ç¯®å­

# ==========================================
# 4. æ™ºèƒ½æé†’ï¼ˆæœ‰å¤§äº‹æ‰å‘é‚®ä»¶ï¼‰
# ==========================================
if new_stories_for_email:  # å¦‚æœæ–°å†…å®¹ç¯®å­é‡Œä¸ä¸ºç©º
    print(f"å‘ç° {len(new_stories_for_email)} æ¡æ–°åŠ¨æ€ï¼Œæ­£åœ¨å‘é€æé†’...")
    email_body = "ğŸï¸ å›´åœºå‰æ–¹æœ‰æ–°æ¶ˆæ¯ï¼š\n\n"
    for item in new_stories_for_email:
        email_body += f"ã€{item['title']}ã€‘\nğŸ”— ä¼ é€é—¨ï¼š{item['link']}\n\n"
    
    try:
        msg = MIMEText(email_body)  # å°è£…é‚®ä»¶å†…å®¹
        msg['Subject'] = f'ğŸ”¥ F1 å®æ—¶æ›´æ–°ï¼š{len(new_stories_for_email)}æ¡æ–°èµ„è®¯'
        msg['From'] = os.getenv('RECEIVER_EMAIL')  # ä»ç¯å¢ƒå˜é‡è¯»å–å‘ä»¶äºº
        msg['To'] =os.getenv('RECEIVER_EMAIL')   # <--- åœ¨è¿™é‡Œå¡«å…¥ä½ è‡ªå·±çš„é‚®ç®±

        with smtplib.SMTP_SSL('smtp.qq.com', 465) as server:  # è¿æ¥é‚®ä»¶æœåŠ¡å™¨
            server.login(os.getenv('RECEIVER_EMAIL'), os.getenv('EMAIL_PASS'))  # ç™»å½•
            server.send_message(msg)  # å‘é€
        print("âœ… é‚®ä»¶å·²æˆåŠŸé€è¾¾ï¼")
    except Exception as e:
        print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
else:
    print("ğŸ’¤ å›´åœºæ²¡æœ‰æ–°åŠ¨ä½œï¼Œä¿æŒå®‰é™ã€‚")

# ==========================================
# 5. æ›´æ–°æ¡£æ¡ˆï¼ˆè¦†ç›–ä¿å­˜ï¼‰
# ==========================================
with open(filename, 'w', encoding='utf-8-sig', newline='') as f:  # ä½¿ç”¨ 'w' æ¨¡å¼è¦†ç›–å†™å…¥
    writer = csv.writer(f)
    writer.writerow(['å‘å¸ƒæ—¶é—´', 'ç±»å‹/æ¥æº', 'æ ‡é¢˜', 'ç®€è®¯'])  # å†™ä¸‹è¡¨å¤´
    writer.writerows(current_scraped_data)  # ä¸€å£æ°”å†™å…¥ä»Šå¤©æŠ“åˆ°çš„æ‰€æœ‰æ•°æ®

print(f"ğŸ“Š å®æ—¶æ¡£æ¡ˆå·²åŒæ­¥ï¼š{filename}")